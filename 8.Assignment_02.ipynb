{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6434d57-e810-4a0a-8bc4-6775fd519a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "Go to this given URL and solve the following questions\n",
    "URL: https://www.youtube.com/@PW-Foundation/videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d552b7-1973-40a5-afae-d5c48a5480c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1. Write a python program to extract the video URL of the first five videos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "385d231e-5d29-48a4-9f33-18bb17fb269e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (2.31.0)\n",
      "Requirement already satisfied: beautifulsoup4 in /opt/conda/lib/python3.10/site-packages (4.11.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests) (2022.12.7)\n",
      "Requirement already satisfied: soupsieve>1.2 in /opt/conda/lib/python3.10/site-packages (from beautifulsoup4) (2.3.2.post1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install requests beautifulsoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "657e87f1-5068-42d3-a461-10d5736b6280",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# URL of the YouTube channel videos page\n",
    "channel_url = 'https://www.youtube.com/@PW-Foundation/videos'\n",
    "\n",
    "# Send a GET request to the channel's videos page\n",
    "response = requests.get(channel_url)\n",
    "response.raise_for_status()  # Ensure the request was successful\n",
    "\n",
    "# Parse the HTML content of the page\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "# Find all the video links on the page\n",
    "video_tags = soup.find_all('a', {'id': 'thumbnail'})\n",
    "\n",
    "# Extract the first five video URLs\n",
    "base_url = 'https://www.youtube.com'\n",
    "video_urls = [base_url + video_tag['href'] for video_tag in video_tags[:5] if video_tag.get('href')]\n",
    "\n",
    "# Print the video URLs\n",
    "for url in video_urls:\n",
    "    print(url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "162a1615-7a0a-4d7c-be65-6fe274436287",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q2. Write a python program to extract the URL of the video thumbnails of the first five videos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e8358c48-1c95-4650-98df-cb14af0abf2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# URL of the YouTube channel videos page\n",
    "channel_url = 'https://www.youtube.com/@PW-Foundation/videos'\n",
    "\n",
    "# Send a GET request to the channel's videos page\n",
    "response = requests.get(channel_url)\n",
    "response.raise_for_status()  # Ensure the request was successful\n",
    "\n",
    "# Parse the HTML content of the page\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "# Find all the video links on the page\n",
    "video_tags = soup.find_all('a', {'id': 'thumbnail'})\n",
    "\n",
    "# Extract the first five video thumbnail URLs\n",
    "thumbnail_urls = []\n",
    "for video_tag in video_tags[:5]:\n",
    "    img_tag = video_tag.find('img')\n",
    "    if img_tag and 'src' in img_tag.attrs:\n",
    "        thumbnail_url = img_tag['src']\n",
    "        thumbnail_urls.append(thumbnail_url)\n",
    "\n",
    "# Print the thumbnail URLs\n",
    "for url in thumbnail_urls:\n",
    "    print(url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a4757aa-e61c-4dba-be06-c60b1a2f3d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q3. Write a python program to extract the title of the first five videos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4c3142b-2b1f-4be2-ab73-9a4ecd585a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# URL of the YouTube channel videos page\n",
    "channel_url = 'https://www.youtube.com/@PW-Foundation/videos'\n",
    "\n",
    "# Send a GET request to the channel's videos page\n",
    "response = requests.get(channel_url)\n",
    "response.raise_for_status()  # Ensure the request was successful\n",
    "\n",
    "# Parse the HTML content of the page\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "# Find all the video links on the page\n",
    "video_tags = soup.find_all('a', {'id': 'thumbnail'})\n",
    "\n",
    "# Extract the titles of the first five videos\n",
    "video_titles = []\n",
    "for video_tag in video_tags[:5]:\n",
    "    # Find the title element inside the video tag\n",
    "    title_tag = video_tag.find('img')\n",
    "    if title_tag and 'alt' in title_tag.attrs:\n",
    "        video_titles.append(title_tag['alt'])\n",
    "\n",
    "# Print the video titles\n",
    "for title in video_titles:\n",
    "    print(title)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5389928-3a9c-4f82-8e5a-96777ba3c4bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q4. Write a python program to extract the number of views of the first five videos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08bbfb58-7373-4e23-8fc1-4596fc5c8176",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# URL of the YouTube channel videos page\n",
    "channel_url = 'https://www.youtube.com/@PW-Foundation/videos'\n",
    "\n",
    "# Send a GET request to the channel's videos page\n",
    "response = requests.get(channel_url)\n",
    "response.raise_for_status()  # Ensure the request was successful\n",
    "\n",
    "# Parse the HTML content of the page\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "# Find all the video links on the page\n",
    "video_tags = soup.find_all('a', {'id': 'thumbnail'})\n",
    "\n",
    "# Extract the number of views for the first five videos\n",
    "view_counts = []\n",
    "for video_tag in video_tags[:5]:\n",
    "    parent_div = video_tag.find_parent('div', {'id': 'dismissible'})\n",
    "    if parent_div:\n",
    "        view_span = parent_div.find('span', class_='inline-metadata-item style-scope ytd-video-meta-block')\n",
    "        if view_span:\n",
    "            view_counts.append(view_span.text.strip())\n",
    "\n",
    "# Print the view counts\n",
    "for count in view_counts:\n",
    "    print(count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec7a95f0-4b94-4835-bad8-ee978e661a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q5. Write a python program to extract the time of posting of video for the first five videos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6c2f66f-70cd-4cca-8685-4dbaebb37ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# URL of the YouTube channel videos page\n",
    "channel_url = 'https://www.youtube.com/@PW-Foundation/videos'\n",
    "\n",
    "# Send a GET request to the channel's videos page\n",
    "response = requests.get(channel_url)\n",
    "response.raise_for_status()  # Ensure the request was successful\n",
    "\n",
    "# Parse the HTML content of the page\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "# Find all the video links on the page\n",
    "video_tags = soup.find_all('a', {'id': 'thumbnail'})\n",
    "\n",
    "# Extract the time of posting for the first five videos\n",
    "posting_times = []\n",
    "for video_tag in video_tags[:5]:\n",
    "    parent_div = video_tag.find_parent('div', {'id': 'dismissible'})\n",
    "    if parent_div:\n",
    "        time_span = parent_div.find('span', class_='style-scope ytd-grid-video-renderer')\n",
    "        if time_span:\n",
    "            posting_times.append(time_span.text.strip())\n",
    "\n",
    "# Print the posting times\n",
    "for time in posting_times:\n",
    "    print(time)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
