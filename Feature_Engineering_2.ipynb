{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Q1. What is the Filter method in feature selection, and how does it work"
      ],
      "metadata": {
        "id": "wqnvQnXJGG_r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "What is Feature Selection? Feature selection involves choosing and retaining only the most important features in a model. Unlike feature extraction, which creates new features from existing ones, feature selection focuses on subsetting existing features."
      ],
      "metadata": {
        "id": "cVnZVuMRGKd-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Why is Feature Selection Important?\n",
        "Simplifies the model by reducing data dimensions, improving visualization, and adhering to Occam‚Äôs razor.\n",
        "Reduces training time and avoids overfitting.\n",
        "Enhances model accuracy and prevents the curse of dimensionality."
      ],
      "metadata": {
        "id": "pihrCJHeGUjt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Filter Method:\n",
        "\n",
        "In this method, features are filtered based on general characteristics (e.g., correlation) with the dependent variable.\n",
        "It doesn‚Äôt involve a predictive model, making it faster.\n",
        "Ideal for scenarios with a large number of features.\n",
        "While it avoids overfitting, it may not always select the best features."
      ],
      "metadata": {
        "id": "2BWgJJkeGctm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Comparison of Methods:\n",
        "\n",
        "Filter Method: Fast, useful for many features.\n",
        "\n",
        "Wrapper Method: Better performance but computationally expensive.\n",
        "\n",
        "Embedded Method: Lies between the other two methods1.\n",
        "\n",
        "Remember, filter methods evaluate features independently based on statistical measures, ranking them by score before selecting or removing them from the dataset2. If you have any more questions, feel free to ask! üòä\n",
        "\n"
      ],
      "metadata": {
        "id": "-AnNNDv-GnnW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q2. How does the Wrapper method differ from the Filter method in feature selection?"
      ],
      "metadata": {
        "id": "47nsDgLVGnpZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Filter Method:\n",
        "Objective: The filter method selects features based on their general characteristics (e.g., correlation) with the dependent variable.\n",
        "Process:\n",
        "Features are evaluated independently using univariate statistics (e.g., correlation coefficients).\n",
        "No predictive model is involved.\n",
        "Faster approach, especially for a large number of features.\n",
        "Pros:\n",
        "Avoids overfitting.\n",
        "Quick and efficient.\n",
        "Cons:\n",
        "May not always select the best features."
      ],
      "metadata": {
        "id": "wszAEeHSHXO2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Wrapper Method:\n",
        "Objective: The wrapper method measures the ‚Äúusefulness‚Äù of features based on classifier performance.\n",
        "Process:\n",
        "Features are evaluated by training a model on different subsets of features.\n",
        "Computationally expensive due to model training.\n",
        "Prone to overfitting.\n",
        "Pros:\n",
        "Better performance compared to filter method.\n",
        "Cons:\n",
        "Higher computational cost.\n",
        "In summary, the filter method focuses on intrinsic properties of features (relevance), while the wrapper method assesses usefulness based on model performance"
      ],
      "metadata": {
        "id": "nLgJNIuNH3Yp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q3. What are some common techniques used in Embedded feature selection"
      ],
      "metadata": {
        "id": "E9VXHEQmH8oT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Embedded feature selection methods are a powerful way to enhance machine learning models by automatically selecting relevant features during the model training process. These methods ‚Äúembed‚Äù the feature selection procedure within the model building phase. Here are some common embedded techniques:"
      ],
      "metadata": {
        "id": "WBOQqhQ9IApf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lasso Regularization (L1 Regularization):\n",
        "Lasso is commonly used in linear regression models. It predicts outcomes based on a linear combination of features.\n",
        "The key idea is to minimize the squared difference between the actual and predicted target values while also penalizing the absolute values of the feature coefficients.\n",
        "Lasso encourages sparsity by shrinking some coefficients to zero, effectively selecting a subset of important features.\n",
        "It‚Äôs particularly useful when dealing with high-dimensional data.\n",
        "Computational cost: Equivalent to the model training time1"
      ],
      "metadata": {
        "id": "ZeEphi2WIbwZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Feature Importance from Decision Trees:\n",
        "Decision trees provide a natural way to assess feature importance.\n",
        "During tree construction, features are split based on their ability to reduce impurity (e.g., Gini impurity or information gain).\n",
        "The importance of a feature is determined by how much it contributes to overall impurity reduction across all splits.\n",
        "Random Forests and Gradient Boosting models use ensemble techniques to aggregate feature importances from multiple decision trees.\n",
        "Computational cost: Depends on the complexity of the tree-building process.\n",
        "Remember that not all machine learning models naturally embed a feature selection process. For instance, support vector machines (SVMs) do not inherently perform feature selection as part of their training process. However, Lasso and feature importance from decision trees are widely applicable and effective methods for embedded feature selection"
      ],
      "metadata": {
        "id": "XIqW4p08Iklv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q4. What are some drawbacks of using the Filter method for feature selection?"
      ],
      "metadata": {
        "id": "FqMp88ClIsFL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Filter method for feature selection has its advantages, but it also comes with limitations. Here are some drawbacks:\n",
        "\n",
        "Independence Assumption: Filter methods rank features independently of each other. They don‚Äôt consider interactions between features. As a result, redundant variables may not be eliminated effectively1.\n",
        "Lack of Predictive Model: Filter methods operate without building a predictive model. While this makes them faster, it can lead to suboptimal feature selection. Sometimes, they may fail to select the best features2.\n",
        "Limited Context: Filter methods don‚Äôt take into account the specific learning task or the model being used. They treat all features equally, which might not be ideal for certain problems3.\n",
        "Data-Driven Limitations: If there isn‚Äôt enough data to model statistical correlations between features, filter methods may perform worse than wrapper methods. However, they are less prone to overfitting3.\n",
        "Remember that the choice of feature selection method depends on the dataset, problem, and computational resources available. It‚Äôs essential to weigh the trade-offs carefully when selecting a method."
      ],
      "metadata": {
        "id": "_Wy2EEqzJL4U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q5. In which situations would you prefer using the Filter method over the Wrapper method for feature\n",
        "selection?"
      ],
      "metadata": {
        "id": "O_fWV7BdJeR-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The choice between using the Filter method and the Wrapper method for feature selection largely depends on the specific characteristics and constraints of the problem you're addressing. Here are some situations where you might prefer using the Filter method over the Wrapper method:\n",
        "\n",
        "1. **Large Datasets**:\n",
        "   - **Filter Method**: When dealing with very large datasets, the Filter method is generally more efficient because it does not involve training a model multiple times. It uses statistical techniques to evaluate the relevance of features, making it faster and more scalable.\n",
        "   - **Wrapper Method**: The Wrapper method, on the other hand, involves training and evaluating models for different subsets of features, which can be computationally expensive and time-consuming, especially for large datasets.\n",
        "\n",
        "2. **High Dimensionality**:\n",
        "   - **Filter Method**: When the number of features is very high, the Filter method can quickly eliminate irrelevant or redundant features based on statistical measures without having to train a model. This makes it suitable for high-dimensional data.\n",
        "   - **Wrapper Method**: The computational cost of the Wrapper method increases exponentially with the number of features, making it impractical for datasets with very high dimensionality.\n",
        "\n",
        "3. **Speed and Efficiency**:\n",
        "   - **Filter Method**: If the primary concern is speed and computational efficiency, the Filter method is preferred as it is generally faster and less resource-intensive. This is important in real-time applications or when quick iterations are needed.\n",
        "   - **Wrapper Method**: While the Wrapper method can provide more accurate feature selection, it is slower due to the need to repeatedly train and evaluate the model.\n",
        "\n",
        "4. **Independence from Model Choice**:\n",
        "   - **Filter Method**: The Filter method is model-agnostic, meaning it does not depend on the choice of the learning algorithm. It selects features based on their intrinsic properties and their relationships with the target variable.\n",
        "   - **Wrapper Method**: The Wrapper method is model-dependent, as it selects features based on their performance with a specific learning algorithm. This makes the selection process specific to the chosen model.\n",
        "\n",
        "5. **Preliminary Feature Selection**:\n",
        "   - **Filter Method**: It can be used as a preliminary step to quickly reduce the number of features before applying more computationally intensive methods, such as the Wrapper method.\n",
        "   - **Wrapper Method**: It is typically used after a preliminary reduction in the number of features or when computational resources are not a constraint.\n",
        "\n",
        "In summary, the Filter method is preferred in situations where you need a fast, efficient, and model-agnostic approach to feature selection, especially when dealing with large datasets, high dimensionality, or when performing a preliminary feature reduction. The Wrapper method, while potentially more accurate, is better suited for smaller datasets and situations where computational resources and time are not primary constraints."
      ],
      "metadata": {
        "id": "g_6waWu8JinF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q6. In a telecom company, you are working on a project to develop a predictive model for customer churn.\n",
        "You are unsure of which features to include in the model because the dataset contains several different\n",
        "ones. Describe how you would choose the most pertinent attributes for the model using the Filter Method."
      ],
      "metadata": {
        "id": "ox4euBojKTMj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To choose the most pertinent attributes for a predictive model for customer churn in a telecom company using the Filter Method, you would follow a systematic process to evaluate and select features based on statistical measures. Here's a step-by-step approach:\n",
        "\n",
        "### Step 1: Understand the Dataset\n",
        "- **Data Exploration**: Begin by exploring the dataset to understand the features, their types (numerical, categorical), distributions, and the presence of any missing values.\n",
        "- **Target Variable**: Identify the target variable, which in this case is customer churn (e.g., churned vs. not churned).\n",
        "\n",
        "### Step 2: Preprocess the Data\n",
        "- **Handle Missing Values**: Impute or remove missing values as appropriate.\n",
        "- **Encode Categorical Variables**: Convert categorical variables to numerical representations using techniques like one-hot encoding or label encoding.\n",
        "\n",
        "### Step 3: Apply Statistical Techniques for Feature Selection\n",
        "- **Correlation Analysis (for Numerical Features)**:\n",
        "  - Compute the correlation matrix to evaluate the correlation between numerical features and the target variable.\n",
        "  - Select features that have a significant correlation with the target variable. Features with low or no correlation can be discarded.\n",
        "  - Be mindful of multicollinearity. If two features are highly correlated with each other, consider keeping only one of them to avoid redundancy.\n",
        "\n",
        "- **Chi-Squared Test (for Categorical Features)**:\n",
        "  - Use the Chi-Squared test to determine the association between categorical features and the target variable.\n",
        "  - Features with a high Chi-Squared statistic (indicating a strong relationship with the target) are considered important.\n",
        "\n",
        "- **ANOVA F-test (for Numerical Features with Categorical Target)**:\n",
        "  - Perform ANOVA F-tests to compare the means of numerical features across different classes of the target variable.\n",
        "  - Features with a high F-statistic indicate significant differences between classes and are deemed relevant.\n",
        "\n",
        "### Step 4: Rank and Select Features\n",
        "- **Rank Features**: Based on the statistical tests performed, rank the features according to their relevance to the target variable.\n",
        "- **Threshold Setting**: Decide on a threshold or the number of top features to select based on their statistical significance.\n",
        "\n",
        "### Step 5: Validate Selected Features\n",
        "- **Model Performance Evaluation**: Build a preliminary model using the selected features and evaluate its performance using cross-validation.\n",
        "- **Iterative Refinement**: Iteratively refine the feature set by adding or removing features based on model performance and business knowledge.\n",
        "\n",
        "### Step 6: Finalize the Feature Set\n",
        "- **Business Insights**: Incorporate domain knowledge and business insights to ensure that the selected features make sense in the context of customer churn.\n",
        "- **Final Selection**: Finalize the feature set that balances statistical significance, model performance, and business relevance.\n",
        "\n",
        "### Example Implementation:\n",
        "Let's say your dataset contains features like `tenure`, `monthly_charges`, `total_charges`, `contract_type`, `internet_service`, and `customer_support_calls`.\n",
        "\n",
        "1. **Correlation Analysis**:\n",
        "   - Calculate the correlation between `tenure`, `monthly_charges`, `total_charges`, and the target variable `churn`.\n",
        "   - Select features with significant correlation, e.g., `tenure` and `monthly_charges`.\n",
        "\n",
        "2. **Chi-Squared Test**:\n",
        "   - Conduct Chi-Squared tests for `contract_type`, `internet_service`, and `customer_support_calls` against `churn`.\n",
        "   - Features like `contract_type` and `customer_support_calls` might show a strong association.\n",
        "\n",
        "3. **ANOVA F-test**:\n",
        "   - Perform ANOVA F-tests for numerical features with the categorical target `churn`.\n",
        "   - Identify significant features, possibly confirming the importance of `monthly_charges`.\n",
        "\n",
        "4. **Rank and Select**:\n",
        "   - Rank features based on their statistical test results.\n",
        "   - Select the top features like `tenure`, `monthly_charges`, `contract_type`, and `customer_support_calls`.\n",
        "\n",
        "5. **Validate and Finalize**:\n",
        "   - Build and evaluate a model using these features.\n",
        "   - Refine the feature set based on model performance and finalize the selection.\n",
        "\n",
        "By following this structured approach, you can effectively use the Filter Method to select the most pertinent attributes for your customer churn predictive model."
      ],
      "metadata": {
        "id": "V3YFK5fpKl84"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q7. You are working on a project to predict the outcome of a soccer match. You have a large dataset with\n",
        "many features, including player statistics and team rankings. Explain how you would use the Embedded\n",
        "method to select the most relevant features for the model."
      ],
      "metadata": {
        "id": "Cw41_Mm9LC4Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Embedded method for feature selection involves using machine learning algorithms that have built-in feature selection mechanisms. These algorithms incorporate feature selection as part of the model training process. Some commonly used algorithms for the Embedded method are regularization techniques like Lasso (L1 regularization), Ridge (L2 regularization), Elastic Net (combination of L1 and L2), and tree-based methods like Random Forest and Gradient Boosting.\n",
        "\n",
        "Here‚Äôs how you can use the Embedded method to select the most relevant features for predicting the outcome of a soccer match:\n",
        "\n",
        "### Step 1: Understand the Dataset\n",
        "- **Data Exploration**: Begin by exploring the dataset to understand the features, their types, distributions, and the presence of any missing values.\n",
        "- **Target Variable**: Identify the target variable, which in this case is the outcome of the soccer match (e.g., win, lose, draw).\n",
        "\n",
        "### Step 2: Preprocess the Data\n",
        "- **Handle Missing Values**: Impute or remove missing values as appropriate.\n",
        "- **Encode Categorical Variables**: Convert categorical variables to numerical representations using techniques like one-hot encoding or label encoding.\n",
        "\n",
        "### Step 3: Split the Data\n",
        "- **Train-Test Split**: Split the dataset into training and testing sets to evaluate model performance after feature selection.\n",
        "\n",
        "### Step 4: Choose an Embedded Method\n",
        "- **Regularization Techniques**: Use algorithms like Lasso, Ridge, or Elastic Net if you have many features and suspect that only a subset of them are important.\n",
        "- **Tree-Based Methods**: Use algorithms like Random Forest or Gradient Boosting which provide feature importance scores.\n",
        "\n",
        "### Step 5: Implement the Embedded Method\n",
        "Below is an example using Lasso for feature selection, but you can adapt it for other embedded methods as needed.\n",
        "\n",
        "```python\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import Lasso\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "import numpy as np\n",
        "\n",
        "# Load the dataset\n",
        "# Assume the dataset is in a CSV file named 'soccer_data.csv'\n",
        "data = pd.read_csv('soccer_data.csv')\n",
        "\n",
        "# Initial data exploration\n",
        "print(data.head())\n",
        "print(data.info())\n",
        "print(data.describe())\n",
        "\n",
        "# Identify features and target variable\n",
        "features = data.drop(columns=['match_outcome'])  # Replace 'match_outcome' with the actual target column name\n",
        "target = data['match_outcome']\n",
        "\n",
        "# Handle missing values\n",
        "features.fillna(features.median(), inplace=True)\n",
        "target.fillna(target.mode()[0], inplace=True)\n",
        "\n",
        "# Encode categorical variables\n",
        "features = pd.get_dummies(features, drop_first=True)\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.3, random_state=42)\n",
        "\n",
        "# Feature selection using Lasso (L1 Regularization)\n",
        "lasso = Lasso(alpha=0.01, random_state=42)\n",
        "lasso.fit(X_train, y_train)\n",
        "\n",
        "# Get the coefficients of the features\n",
        "lasso_coefficients = pd.Series(lasso.coef_, index=X_train.columns)\n",
        "selected_features = lasso_coefficients[lasso_coefficients != 0].index\n",
        "print(\"Selected Features using Lasso:\", selected_features)\n",
        "\n",
        "# Train a model using the selected features\n",
        "X_train_selected = X_train[selected_features]\n",
        "X_test_selected = X_test[selected_features]\n",
        "\n",
        "# Train a Random Forest model to evaluate feature selection\n",
        "model = RandomForestClassifier(random_state=42)\n",
        "model.fit(X_train_selected, y_train)\n",
        "y_pred = model.predict(X_test_selected)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Model Accuracy with Selected Features:\", accuracy)\n",
        "```\n",
        "\n",
        "### Explanation:\n",
        "1. **Data Loading and Exploration**:\n",
        "   - Load the dataset and perform basic exploration to understand the features and target variable.\n",
        "2. **Preprocessing**:\n",
        "   - Handle missing values.\n",
        "   - Encode categorical variables using `get_dummies`.\n",
        "3. **Train-Test Split**:\n",
        "   - Split the dataset into training and testing sets.\n",
        "4. **Feature Selection using Lasso**:\n",
        "   - Apply Lasso regression to identify and select the most relevant features.\n",
        "   - Extract the non-zero coefficients from Lasso as the selected features.\n",
        "5. **Model Training and Evaluation**:\n",
        "   - Train a `RandomForestClassifier` using the selected features.\n",
        "   - Evaluate the model‚Äôs accuracy to validate the selected features.\n",
        "\n",
        "### Alternative Embedded Methods:\n",
        "- **Random Forest/Gradient Boosting**: You can use feature importances from tree-based methods as another Embedded method for feature selection.\n",
        "\n",
        "```python\n",
        "# Feature selection using Random Forest\n",
        "rf = RandomForestClassifier(random_state=42)\n",
        "rf.fit(X_train, y_train)\n",
        "\n",
        "# Get feature importances\n",
        "importances = rf.feature_importances_\n",
        "feature_importance_df = pd.DataFrame({'Feature': X_train.columns, 'Importance': importances})\n",
        "selected_features_rf = feature_importance_df[feature_importance_df['Importance'] > 0.01]['Feature']  # Threshold for importance\n",
        "print(\"Selected Features using Random Forest:\", selected_features_rf)\n",
        "\n",
        "# Train a model using the selected features\n",
        "X_train_selected_rf = X_train[selected_features_rf]\n",
        "X_test_selected_rf = X_test[selected_features_rf]\n",
        "\n",
        "# Train a Random Forest model to evaluate feature selection\n",
        "model_rf = RandomForestClassifier(random_state=42)\n",
        "model_rf.fit(X_train_selected_rf, y_train)\n",
        "y_pred_rf = model_rf.predict(X_test_selected_rf)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy_rf = accuracy_score(y_test, y_pred_rf)\n",
        "print(\"Model Accuracy with Selected Features (Random Forest):\", accuracy_rf)\n",
        "```\n",
        "\n",
        "Using these approaches, you can effectively apply the Embedded method to select the most relevant features for your soccer match outcome prediction model. Adjust the dataset path, target variable, and other parameters as needed for your specific use case."
      ],
      "metadata": {
        "id": "S4EbV_ZTSs_K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q8. You are working on a project to predict the price of a house based on its features, such as size, location,\n",
        "and age. You have a limited number of features, and you want to ensure that you select the most important\n",
        "ones for the model. Explain how you would use the Wrapper method to select the best set of features for the\n",
        "predictor.\n"
      ],
      "metadata": {
        "id": "oaxTHBySSuWM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To use the Wrapper method for feature selection in predicting house prices, you follow an iterative process where you train and evaluate a model using different subsets of features. The Wrapper method typically involves techniques like Forward Selection, Backward Elimination, or Recursive Feature Elimination (RFE).\n",
        "\n",
        "Here's a step-by-step guide to using the Wrapper method:\n",
        "\n",
        "### Step 1: Understand the Dataset\n",
        "- **Data Exploration**: Explore the dataset to understand the features, their types (numerical, categorical), distributions, and the presence of any missing values.\n",
        "- **Target Variable**: Identify the target variable, which in this case is the house price.\n",
        "\n",
        "### Step 2: Preprocess the Data\n",
        "- **Handle Missing Values**: Impute or remove missing values as appropriate.\n",
        "- **Encode Categorical Variables**: Convert categorical variables to numerical representations using techniques like one-hot encoding or label encoding.\n",
        "- **Normalize/Standardize Features**: Depending on the model, normalize or standardize the features to ensure they are on a comparable scale.\n",
        "\n",
        "### Step 3: Split the Data\n",
        "- **Train-Test Split**: Split the dataset into training and testing sets to evaluate model performance after feature selection.\n",
        "\n",
        "### Step 4: Choose a Wrapper Method\n",
        "- **Forward Selection**: Start with no features and add one feature at a time, evaluating the model performance at each step.\n",
        "- **Backward Elimination**: Start with all features and remove one feature at a time, evaluating the model performance at each step.\n",
        "- **Recursive Feature Elimination (RFE)**: Use a model to rank features by importance and recursively eliminate the least important features.\n",
        "\n",
        "### Step 5: Implement the Wrapper Method\n",
        "Below is an example using Recursive Feature Elimination (RFE) with a linear regression model:\n",
        "\n",
        "```python\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.feature_selection import RFE\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Load the dataset\n",
        "# Assume the dataset is in a CSV file named 'house_prices.csv'\n",
        "data = pd.read_csv('house_prices.csv')\n",
        "\n",
        "# Initial data exploration\n",
        "print(data.head())\n",
        "print(data.info())\n",
        "print(data.describe())\n",
        "\n",
        "# Identify features and target variable\n",
        "features = data.drop(columns=['price'])  # Replace 'price' with the actual target column name\n",
        "target = data['price']\n",
        "\n",
        "# Handle missing values\n",
        "features.fillna(features.median(), inplace=True)\n",
        "target.fillna(target.median(), inplace=True)\n",
        "\n",
        "# Encode categorical variables\n",
        "features = pd.get_dummies(features, drop_first=True)\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.3, random_state=42)\n",
        "\n",
        "# Initialize the model\n",
        "model = LinearRegression()\n",
        "\n",
        "# Recursive Feature Elimination (RFE)\n",
        "# Selecting 5 features for the example; adjust based on your needs\n",
        "n_features_to_select = 5\n",
        "rfe = RFE(model, n_features_to_select)\n",
        "rfe.fit(X_train, y_train)\n",
        "\n",
        "# Get the selected features\n",
        "selected_features = X_train.columns[rfe.support_]\n",
        "print(\"Selected Features:\", selected_features)\n",
        "\n",
        "# Train the model with selected features\n",
        "X_train_selected = X_train[selected_features]\n",
        "X_test_selected = X_test[selected_features]\n",
        "\n",
        "model.fit(X_train_selected, y_train)\n",
        "y_pred = model.predict(X_test_selected)\n",
        "\n",
        "# Evaluate the model\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(\"Model Mean Squared Error with Selected Features:\", mse)\n",
        "```\n",
        "\n",
        "### Explanation:\n",
        "1. **Data Loading and Exploration**:\n",
        "   - Load the dataset and perform basic exploration to understand the features and target variable.\n",
        "2. **Preprocessing**:\n",
        "   - Handle missing values.\n",
        "   - Encode categorical variables using `get_dummies`.\n",
        "3. **Train-Test Split**:\n",
        "   - Split the dataset into training and testing sets.\n",
        "4. **Feature Selection using RFE**:\n",
        "   - Apply Recursive Feature Elimination (RFE) with a linear regression model to select the most important features.\n",
        "   - Extract the selected features based on the RFE ranking.\n",
        "5. **Model Training and Evaluation**:\n",
        "   - Train a linear regression model using the selected features.\n",
        "   - Evaluate the model‚Äôs mean squared error (MSE) to validate the selected features.\n",
        "\n",
        "### Alternative Wrapper Methods:\n",
        "- **Forward Selection**:\n",
        "  ```python\n",
        "  from sklearn.feature_selection import SequentialFeatureSelector\n",
        "  \n",
        "  sfs = SequentialFeatureSelector(model, n_features_to_select=5, direction='forward')\n",
        "  sfs.fit(X_train, y_train)\n",
        "  \n",
        "  selected_features_fs = X_train.columns[sfs.get_support()]\n",
        "  print(\"Selected Features using Forward Selection:\", selected_features_fs)\n",
        "  \n",
        "  # Train the model with selected features\n",
        "  X_train_selected_fs = X_train[selected_features_fs]\n",
        "  X_test_selected_fs = X_test[selected_features_fs]\n",
        "  \n",
        "  model.fit(X_train_selected_fs, y_train)\n",
        "  y_pred_fs = model.predict(X_test_selected_fs)\n",
        "  \n",
        "  mse_fs = mean_squared_error(y_test, y_pred_fs)\n",
        "  print(\"Model Mean Squared Error with Selected Features (Forward Selection):\", mse_fs)\n",
        "  ```\n",
        "\n",
        "- **Backward Elimination**:\n",
        "  ```python\n",
        "  sfs_backward = SequentialFeatureSelector(model, n_features_to_select=5, direction='backward')\n",
        "  sfs_backward.fit(X_train, y_train)\n",
        "  \n",
        "  selected_features_be = X_train.columns[sfs_backward.get_support()]\n",
        "  print(\"Selected Features using Backward Elimination:\", selected_features_be)\n",
        "  \n",
        "  # Train the model with selected features\n",
        "  X_train_selected_be = X_train[selected_features_be]\n",
        "  X_test_selected_be = X_test[selected_features_be]\n",
        "  \n",
        "  model.fit(X_train_selected_be, y_train)\n",
        "  y_pred_be = model.predict(X_test_selected_be)\n",
        "  \n",
        "  mse_be = mean_squared_error(y_test, y_pred_be)\n",
        "  print(\"Model Mean Squared Error with Selected Features (Backward Elimination):\", mse_be)\n",
        "  ```\n",
        "\n",
        "Using these approaches, you can effectively apply the Wrapper method to select the most relevant features for your house price prediction model. Adjust the dataset path, target variable, and other parameters as needed for your specific use case."
      ],
      "metadata": {
        "id": "iPgSDvuETbdf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "NK6YLZ72TsEz"
      }
    }
  ]
}